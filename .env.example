# Kimi-K2.5 RunPod Serverless Environment Variables
# Copy this file to .env and fill in your values

# ===========================================
# REQUIRED: Model Configuration
# ===========================================

# Kimi-K2.5 model identifier
MODEL_NAME=moonshotai/Kimi-K2.5

# Your Hugging Face token (required for model download)
HF_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# ===========================================
# REQUIRED: GPU & Parallelism Settings
# ===========================================

# Number of GPUs for tensor parallelism (Kimi-K2.5 requires 8 GPUs)
TENSOR_PARALLEL_SIZE=8

# Enable expert parallelism for MoE (Mixture of Experts) models
ENABLE_EXPERT_PARALLEL=true

# ===========================================
# REQUIRED: Quantization & Memory
# ===========================================

# FP8 quantization for memory efficiency (recommended for Kimi-K2.5)
QUANTIZATION=fp8

# Maximum context length (Kimi-K2.5 supports up to 131072)
MAX_MODEL_LEN=131072

# Fraction of GPU memory to use (0.0 to 1.0)
GPU_MEMORY_UTILIZATION=0.95

# ===========================================
# REQUIRED: Tool Calling Support
# ===========================================

# Enable automatic tool/function selection
ENABLE_AUTO_TOOL_CHOICE=true

# Parser for Kimi-specific tool call format
TOOL_CALL_PARSER=kimi_k2

# ===========================================
# REQUIRED: Security
# ===========================================

# Trust remote code from Hugging Face (required for Kimi model)
TRUST_REMOTE_CODE=true

# ===========================================
# OPTIONAL: Performance Tuning
# ===========================================

# Maximum number of sequences per iteration
MAX_NUM_SEQS=256

# Maximum concurrent requests per worker
MAX_CONCURRENCY=30

# Enable prefix caching for repeated prompts
# ENABLE_PREFIX_CACHING=true

# ===========================================
# OPTIONAL: Streaming Configuration
# ===========================================

# Maximum batch size for token streaming
# DEFAULT_BATCH_SIZE=50

# Minimum batch size for first request
# DEFAULT_MIN_BATCH_SIZE=1

# Growth factor for dynamic batch size
# DEFAULT_BATCH_SIZE_GROWTH_FACTOR=3

# ===========================================
# OPTIONAL: Logging
# ===========================================

# Disable vLLM stats logging (set to true in production)
# DISABLE_LOG_STATS=false

# Disable request logging
# DISABLE_LOG_REQUESTS=false

# ===========================================
# OPTIONAL: Model Override
# ===========================================

# Override the model name in API responses
# OPENAI_SERVED_MODEL_NAME_OVERRIDE=kimi-k2.5

# ===========================================
# DOCKER BUILD ARGS (used when building image)
# ===========================================

# Base path for model storage
# BASE_PATH=/runpod-volume
